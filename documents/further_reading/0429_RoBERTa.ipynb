{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1484d8",
   "metadata": {},
   "source": [
    "# ALBERT (A Lite BERT)\n",
    "- BERT는 원논문에서 제시한 모델이 아직 undertrain 되어있다고 말할 수 있을 정도로 모델의 크기가 다소 커 학습 시간이나 데이터가 더 많이 요구되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d57c4",
   "metadata": {},
   "source": [
    "❗ BERT에서 다음의 사항을 개선하여 모델의 크기는 줄이고 GLUE, SQuAD, RACE Task에 대해 더 높은 성능을 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aeb713",
   "metadata": {},
   "source": [
    "**Sentence order prediction**  \n",
    "- BERT에서의 Next Sentence Prediction(NSP) 대신 두 문장 간 순서를 맞추는 방식으로 학습 (Masked LM 방식은 동일하게 활용) -> NSP 보다 SQUAD, MNLI, RACE task에서 성능 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ba3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_next = rand() < 0.5 # whether token_b is next to token_a or not\n",
    "\n",
    "tokens_a = self.read_tokens(self.f_pos, len_tokens, True)\n",
    "seek_random_offset(self.f_neg)\n",
    "#f_next = self.f_pos if is_next else self.f_neg\n",
    "f_next = self.f_pos # `f_next` should be next point\n",
    "tokens_b = self.read_tokens(f_next, len_tokens, False)\n",
    "\n",
    "if tokens_a is None or tokens_b is None: # end of file\n",
    "    self.f_pos.seek(0, 0) # reset file pointer\n",
    "    return\n",
    "\n",
    "# SOP, sentence-order prediction\n",
    "instance = (is_next, tokens_a, tokens_b) if is_next \\\n",
    "                    else (is_next, tokens_b, tokens_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e804c",
   "metadata": {},
   "source": [
    "  **Cross-layer parameter sharing**   \n",
    "- Transformer의 각 Layer 간 같은 Parameter를 공유하여 사용 -> 모델 크기 줄임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676182e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embed = Embeddings(cfg)\n",
    "        # Original BERT not used parameter-sharing strategies\n",
    "        # self.blocks = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layers)])\n",
    "\n",
    "        # To used parameter-sharing strategies\n",
    "        self.n_layers = cfg.n_layers\n",
    "        self.attn = MultiHeadedSelfAttention(cfg)\n",
    "        self.proj = nn.Linear(cfg.hidden, cfg.hidden)\n",
    "        self.norm1 = LayerNorm(cfg)\n",
    "        self.pwff = PositionWiseFeedForward(cfg)\n",
    "        self.norm2 = LayerNorm(cfg)\n",
    "        # self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, x, seg, mask):\n",
    "        h = self.embed(x, seg)\n",
    "\n",
    "        for _ in range(self.n_layers):\n",
    "            # h = block(h, mask)\n",
    "            h = self.attn(h, mask)\n",
    "            h = self.norm1(h + self.proj(h))\n",
    "            h = self.norm2(h + self.pwff(h))\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d05837",
   "metadata": {},
   "source": [
    "**Factorized embedding parameterization**   \n",
    "- Input Layer의 Parameter 수를 줄임 -> 모델 크기 줄임  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f16dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"The embedding module from word, position and token_type embeddings.\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Original BERT Embedding (token embedding)\n",
    "        # self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.hidden) \n",
    "        \n",
    "        # factorized embedding\n",
    "        self.tok_embed1 = nn.Embedding(cfg.vocab_size, cfg.embedding)\n",
    "        self.tok_embed2 = nn.Linear(cfg.embedding, cfg.hidden)\n",
    "\n",
    "        # position embedding\n",
    "        self.pos_embed = nn.Embedding(cfg.max_len, cfg.hidden) \n",
    "        # segment(token type) embedding\n",
    "        self.seg_embed = nn.Embedding(cfg.n_segments, cfg.hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705b5d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd90b3a",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca30546",
   "metadata": {},
   "source": [
    "### 기존 BERT의 경우 \n",
    "- Model\n",
    "    - transformer encoder(with Multi-head Attention, FFN) x12\n",
    "- Learning WITH\n",
    "    - Masked Language Modeling(MLM)\n",
    "    - Next Sentence Prediction(NSP)\n",
    "- Optimization \n",
    "    - Adam Optimizer, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon=1e-6$, $L2\\;weight\\;decay=0.01$\n",
    "    - learning rate=1e-4, 10,000 steps warmup (10000스텝까지 1e-4로 linear warmup 이후 다시 감소)\n",
    "    - GELU activation, \n",
    "    - 0.1 dropout on all layers\n",
    "    - batch size 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch)\n",
    "    - 1,000,000 steps (128,000 * 1,000,000 / 3.3B word corpus = 40epochs)\n",
    "- character 단위 BPE tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa3b25",
   "metadata": {},
   "source": [
    "### 반면 RoBERTa의 경우\n",
    "**최적의 하이퍼파라미터로 학습**\n",
    "- 더 많은 data와 더 큰 batch size(약 32배) (계산 비용은 그대로 유지)\n",
    "- Peak learning rate/warm up step .. batch size에 따라 각각 튜닝\n",
    "- Large batch size에서 더 안정적이도록 Adam optimizer에서 $\\beta_2 = 0.98$로 설정\n",
    "- input으로 들어가는 sequence의 length 길이를 항상 full(512)로 설정\n",
    "- Dynamic Masking - MLM 수행 시 매 epoch마다 다른 masking을 활용\n",
    "    + data를 10개 복제하여 각 sequence가 40 epoch에 걸쳐 10가지 방법으로 masking 되도록 처리\n",
    "    + 학습 중 동일한 mask는 4번만 보게 된다. \n",
    "- NSP 제거\n",
    "    + NSP는 사실 문장이 붙어있어서 성능이 좋았던거지, 다음 문장 예측으로 학습해서 성능이 좋았던게 아니다.\n",
    "    + 그냥 모든 문장을 붙여서 최대한 길이를 길게(512)해서 넣으면 성능 더 좋았음\n",
    "- byte단위 BPE tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02882dc3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d7799",
   "metadata": {},
   "source": [
    "# Cross-Lingual Language Model\n",
    "### 데이터가 풍부하지 않은 언어를 위해 transfer learning을 진행하는 cross-lingual transfer learning 분야의 연구\n",
    "### 물론 이렇게 학습하는 데에는 다른 이유도 있을 것이다. 번역이 좀 더 쉬워진다든지 등.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd0b77",
   "metadata": {},
   "source": [
    "# XLM\n",
    "\n",
    "- Unsupervised, relies on monolingual data\n",
    "- Supervised, relies on parallel data.\n",
    "\n",
    "**The model uses the same shared vocabulary for all the languages.**\n",
    "- 여러 언어가 하나의 임베딩 공간을 공유하고 어떤 언어로 쓰인 문장도 해당 임베딩 공간으로 인코딩 되도록 하는 유니버셜 인코더를 만들고자 하였다.\n",
    "=> 비슷하게 알파벳을 쓴다던가 하는 언어들은 embedding space에서 잘 매칭됨\n",
    "- BPE 활용 .. BPE를 활용하면 같은 글자(ex. 알파벳) 쓰는 언어들은 토큰 매칭이 가능하다는 장점\n",
    "- 데이터가 부족한 언어에 transfer learning이 가능\n",
    "  \n",
    "![XLM](https://miro.medium.com/max/1000/1*EcaP5C5UhcCqkeNqvYscTA.png)  \n",
    "  \n",
    "- BERT 구조\n",
    "- **CLM** 일반적인 transformer decoder를 이용한 LM\n",
    "- **MLM** default BERT가 input으로 sequence 2개(pair of sentences)를 활용하는 반면, XLM은 길이만 256이면 그냥 문장 몇개든 사용\n",
    "- **TLM** 위와 같이 서로 다른 두 parallel 문장을 두고 mask함. **모든 토큰은 각 마스크 예측에 관여**\n",
    "- Language embedding token 활용\n",
    "- 일반 BERT에서 pretrained 된 token을 미리 가져온 후 그것으로 학습을 시작했더니 성능이 더 좋았다는 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1d35f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ef903",
   "metadata": {},
   "source": [
    "# XLM-RoBERTa(XLM-R)  >> mBERT\n",
    "## Unsupervised Cross-lingual Representation Learning at Scale\n",
    "[원 논문](https://arxiv.org/pdf/1911.02116.pdf)\n",
    "- Building on the cross-lingual approach that we used with XLM and RoBERTa\n",
    "- It is trained on 2.5T of data across 100 languages data filtered from Common Crawl. \n",
    "- XLM-R achieves state-of-the-arts results on multiple cross lingual benchmarks.\n",
    "- 기존 multi-lingual model인 mBERT에 비해 **데이터양이 적은 언어에서 정확도 23% 향상**\n",
    "\n",
    "<br />\n",
    "\n",
    "- 일단 이전 모델들이 학습할 때 사용한 데이터의 양이 적었음을 지적 => 사용하는 데이터를 엄청나게 늘림\n",
    "- 단일 언어에 대한 성능에서도 경쟁력 있음(RoBERTa와 견줄만 함)\n",
    "- Subword tokenization .. SentencePiece with unigram\n",
    "- 250K vocab size\n",
    "- 주로 모델 자체의 구조보다는 어떤 데이터로 학습했으며 성능은 어땠는지를 설명하고 있음.\n",
    "    + monolingual의 성능에 꿇리지 않고, 오히려 cross-lingual(XLM 기반) 모델이 성능을 앞지를 때도 있음을 보이며 가능성을 제시\n",
    "- RoBERTa 백본"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fea9bf",
   "metadata": {},
   "source": [
    "- multi-lingual language modeling을 위해 monolingual data로 학습\n",
    "- 모든 언어에서 text stream을 추출하여 mask token 예측 학습\n",
    "\n",
    "### Curse of Multi-linguality\n",
    "![curse_of_multilinguality](https://miro.medium.com/max/700/1*xQZN1oci9xAKDdA9gBNXGA.png)\n",
    "- While we improve the performance for low-resource languages, this ‘dilution’ degrades the overall performance on the downstream tasks. This dilution has a trade-off with the model capacity.\n",
    "- multilingual 모델 설계의 장점은 데이터가 적은 언어에서도 보다 좋은 성능을 낼 수 있게 된다는 것인데, 이를 위해 언어를 추가하면(dilution) 반대로 overall performance는 떨어지는 tradeoff가 있음.\n",
    "- 하지만 위 그림에서 보이듯이 처음에 성능 증가하다가 하락. 결국 모델 크기 자체를 크게 늘리면 이것도 해결할 수 있음을 제시\n",
    "- 실제로 parameter를 조정하고, low-resource language를 upsampling하고, large shared vocabulary를 생성, capacity 자체를 향상시킴"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
