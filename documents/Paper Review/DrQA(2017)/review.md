## 🤔  리뷰(#54번 이슈)
   
Open-Domain question을 어떤 방식으로 풀어나갈지 포문을 연 논문입니다.  
근데 사실 이 논문 자체에서 큰 아이디어를 얻진 못했던 것 같습니다.. ㅠㅠ

먼저 여기서 제시한 것은 다들 아시다시피
1) Document Retriever 기반 관련 문서 5개 추출
2) Paragraph Encoding/Question Encoding을 통해 문서의 단어별 토큰과 질문의 단어별 토큰을 인코딩하고, 이들 간의 유사도 매칭 및 정답 추출  

위와 같이 저희가 흔히 아는 프로세스로 ODQA를 진행합니다.
논문의 저자는 이와 같은 setting을 **MRS(machine reading at scale**이라 칭합니다. 

여기서 사용한 Retriever 같은 경우 현재 저희가 사용하는 TF-IDF retriever와 동일합니다.
bigram hashing과 TF-IDF matching을 활용하여 top-k=5로 설정하고 5개의 상위 스코어 문서를 추출합니다.

Reader의 경우 당시 transformer 모델이 발표되기 전이라 bi-directional LSTM을 사용합니다.

**여기부터가 사실 이 논문을 읽었던 이유인데요!** 수연님의 리뷰에도 나와있지만, LSTM에 넣어주는 단어별 토큰의 피처벡터는 다음과 같은 여러 성분들을 모두 포함하고 있습니다.
1. Word embedding(300dim, GloVe 기반) + 질문에서 자주 나오는 단어(what, how, ...) 토큰만 추가적으로 fine-tuning
2. Exact match: 1) 단어가 완전히 match하는가? 2) lowercase에서는 match하는가? 3) lemma form(즉, 현재형 과거형 이런거 없앤 형태)에서는 match하는가? (binary feature)
3. Token features: POS, NER 태깅 토큰 + TF(term frequency)
4. aligned question embedding: attention score 반영

제가 보기엔 그 외에는 다 저희가 알고 있는 것과 같고, 이후 내용은 dataset을 어떻게 활용했는지, 성능은 어땠는지에 대해 논의하고 있습니다.

원래 이 논문을  review하게 된 배경이 위에서 명시한 4가지 feature들을 우리도 어떻게 활용할 수 있지 않을까? 에서 시작되었는데요.  

1번의 경우 시도해봄직 하지만 논문에서 "1번으로 인해 성능이 엄청 많이 올랐다" 등의 언급이 없기도 하고(제 생각에도 성능에 큰 영향이 있을 것 같지는 않습니다), 저희가 한국어 기반으로 해당 토큰들을 토크나이저 별로 걸러내서 fine tuning 한다는게 현실적으로 힘들 것 같아 넘기는게 좋을 것 같습니다. 

2번의 경우 저자가 직접적으로 성능이 많이 올랐다고 말할 정도로 critical한 방법이었기 때문에 눈여겨볼만 합니다. 하지만 하나하나 살펴보면 실제로는 이미 적용되고 있거나, 적용할 수 없는 방법입니다. ㅠㅠ

2-1의 경우 이미 TF-IDF Retriever를 통해 22%의 정답 문서를 가져왔을 때 21%에 육박하는 정답률을 보인 것을 보면 MRC 모델 자체에 exact-match에 대한 강건함?은 이미 있는 것으로 보입니다.
2-2의 경우 한국어와는 상관이 없는 것 같구요, 2-3의 경우 역시 한국어 토크나이저에서 어간을 분리하는 것으로 알고 있는데 여기서 어느정도 보장이 되지 않을까 싶습니다. **하지만 2-3의 경우 더 생각해볼만한 것 같으니 의견 있으시면 말씀 부탁드립니다!**

4번의 경우 이제는 transformer 모델을 사용하고 있기 때문에 이미 question과 paragraph간 attention score는 반영되고 있는 상태라 논외인 것 같습니다.

중요한건 3번인데요, 사실 제가 생각하기엔 3번이 제일 critical 한게, ETRI KorBERT 설명에 따르면 실제로 형태소 태깅을 통해 성능을 구글 모델에 비해 5%가량 올렸다고 나와있었기 때문에 저희가 실제로 편하게 적용할 수 있는 방법 같습니다. 논문에서 사용한 3번 방법은 아래와 같이 워드 토큰 자체에 형태소를 붙이는건 아닌거 같긴한데ㅋㅋㅋ 아무튼 같은 효과를 낼 수 있는건 맞습니다. 

ETRI KorBERT의 dictionary 예시입니다.
```
n_iters=10000
max_length=16
[PAD]
[UNK]
[CLS]
[SEP]
[MASK]
<S>
<T>
./SF_	20688850
다/EF_	17194907
하/XSV_	15918761
ㄴ/ETM_	13942053
을/JKO_	13654182
었/EP_	12912273
의/JKG_	12012373
에/JKB_	11360511
이/VCP_	8986000
이/JKS_	8938636
,/SP_	8614178
는/JX_	8594108
를/JKO_	8310256
어/EC_	7491203
은/JX_	7238132
는/ETM_	6614467
고/EC_	6544461
가/JKS_	5607159
"/SS_	5290740
'/SS_	5184635
에서/JKB_	4984600
으로/JKB_	4775583
(/SS_	4605458
)/SS_	4593104
로/JKB_	4052049
되/XSV_	3866969
것/NNB_	3754581
도/JX_	3723164
ㄹ/ETM_	3607940
들/XSN_	3567985
있/VX_	2984347
있/VA_	2766956
년/NNB_	2692800
하/VV_	2640633
ㄴ다/EF_	2593343
하/XSA_	2417945
았/EP_	2248550
일/NNB_	2208697
은/ETM_	2165340
과/JC_	2080158
게/EC_	2045603
지/EC_	2021309
기/ETN_	1982615
1/SN_	1747264
등/NNB_	1741349
자/XSN_	1681880
며/EC_	1662426
2/SN_	1636258
수/NNB_	1612227
와/JC_	1612192
```

위와 같이 형태소 태깅이 들어가있습니다.

리뷰를 급 마무리하는 것 같긴한데, 아무튼 그래서 결론적으로 실제 적용이 가능한 방법들은 이미 적용이 되고 있거나 작업에 비해 성능 향상이 거의 없을 것 같다? 라는 생각이 들고.. 하지만 이건 개인적인 생각이니 비판은 언제나 환영입니다.

그리고 저는 ETRI KorBERT를 사용할 수 있게 세팅하는 것이 우선적으로 필요하다고 생각합니다..! 그래서 오늘 남은시간과 내일동안 저는 ETRI KorBERT를 저희 MRC 모델로 사용할 수 있게 세팅?을 해볼 생각입니다.

---

## 📚  레퍼런스 첨부

[Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/pdf/1704.00051v2.pdf)
