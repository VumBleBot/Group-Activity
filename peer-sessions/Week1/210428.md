# 1주차 수요일 회의 (0428)

## 정보 탐색

* (지영) Follow-up
    * BM25 코드 공유
    * Tokenizer 관련 토론 게시글 리뷰
* (종헌) [Transformer 코드 리뷰](../documents/../../documents/further_reading/0428_transformer.ipynb)
    * Attention 
    * Encoder, Decoder
    * Transformer
    * BERT
* (성익) Tokenizer 정리
    * Sentence Tokenization, Word Tokenization
    * Subword Tokenization (OOV 극복)
    * BPE(Byte Pare Encoding), WordPiece
    * Unigram
    * SentencePiece (= WordPiece Model) 
        * transformers 라이브러리는 모두 Unigram 기반 
        * BPE, WordPiece 기반으로 써보는 것도 좋을 것 같음
    * mecab(한국어)
        * 기존 BERT => mecab 이후 WordPiece 사용해야 성능 향상
        * ALBERT 등 => 처음부터 SentencePiece 사용해도 됨
    * (대회용) SentencePiece 기반 모델 사용이 편함 (ETRI KoBERT)
    * (대회용) Mecab + WordPiece (KoELECTRA v3)
    * SentencePiece BPE Tokenizer >> Google SentencePiece
* (수연) Cluster-Former(2020) 논문 리뷰
    * 현재 SOTA논문
    * transformer의 계산 부담을 줄이기 위한 형태
        * sliding window에서의 transformer는 overlap 부분을 평균내서 계산
        * 그러나 cluster-former의 경우 input을 여러 부분으로 잘라 cluster화하여 hiddenlayer로 만든 뒤에 통과시키고, 마지막에 원래대로 복원
    * 비슷한 의미를 가지는 단어들끼리 clustering하여 attention을 줄 수 있었다.
    * k-means centroid는 epoch마다 업데이트
* (건모) [ODQA(OpenQA) Survey](https://hackmd.io/@cdll-lo-ol-lo-ol/HyWdSdLDO)
    * MRC : 한 doc 안에서 답 찾기
    * ODQA : 정해지지 않은 범위에서 답 찾기
    * Sparse/Dense Retriever
    * Reader

## 베이스라인 파트 분배

* retriever: 2명 - 성익,수연
* train, inference: 2명 - 건모,지영
* utils_qa: 1명 - 종헌

- (금)까지

## 다음 회의

* 1주차 결산
* 다음주 계획
    * 모델, 토크나이저 등 선택
* 사이드프로젝트 회의