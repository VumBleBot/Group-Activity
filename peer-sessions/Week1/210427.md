# 1주차 화요일 회의

## 정보탐색

- (지영) follow-up 정리
- (건모) 수요일 쯤 survey 논문 정리해서 설명
- (수연) Reading Wikipedia to Answer Open Domain Question 논문 요약
    - feature vector 어떻게 추가되는건지 코드 보면서 더 알아보기
- (성익) 수요일까지 베이스라인 코드 tokenizing 부분 + special token을 넣을때 어떤 방식으로 임베딩되는지
    - wordpiece vs sentensepiece 차이
- (종헌) 기초 모델 논문 + 토크나이저

## 베이스라인 코드
- Group-Activity repo에 baseline 코드 커밋해서 질문 생기면 issue 활용


## 모델 구동 결과

| 모델 명 | EM | F1 | 데이터셋 |
| -------- | -------- | -------- | -------- |
| 베이스라인(bert-base-multilingual-cased)     | 0.00     | 10.52     | train |
| bert-base-multilingual-uncased     | 0.00     | 11.81     | train |
| xlm-roberta-base     | 0.00     | 11.17     | train |
| xlm-roberta-large (step 2500)     | 2.50     | 11.32     | train |
| xlm-roberta-large (step 3000)     | 3.30     | 11.96     | train |
| xlm-roberta-large (step 3500)     | 8.33     | 12.70     | train  + valid |
| monologg/koelectra-base-v3-discriminator     | 0.00     | 10.79     | train |
| monologg/koelectra-small-v3-discriminator     | 0.00     | 9.34     | train |
| mrm8488/bert-multi-cased-finetuned-xquadv1   | 0.00     | 13.86     | train |
