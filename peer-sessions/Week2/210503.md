# 2주차 월요일 회의(0503)

TMI 세션 순서
수연 - 성익 - 종헌 - 지영 - 건모

## (건모) EDA, Dense Retriever 구현

### EDA

* MRC 데이터셋 : competition dataset
* KOR 데이터셋 : korquad 데이터셋

| 기준 | MRC 데이터셋 | KOR 데이터셋 |
| -------- | -------- | -------- |
| 문서 길이  | 김. 500 chracter 이하를 인위적으로 자른 느낌 | 짧음. 500 character 이내 문서가 많음. 그냥 500  |
| 정답의 위치 | 상대적으로 뒤쪽 | 앞쪽 |
| 정답의 시작 위치(ans_start) | 비교적 균형 | imablance(앞쪽에 크게 몰려있음) -> 샘플링 논의 필요 |
| 전처리 파이프라인 | 기호 일반화(clean_punc), remove_bad_char 전처리 필요. | 마찬가지. |

- **Token vector 군집화(KoBERT 임베딩)** : train/valid에 겹치는 문서가 있음! data leakeage handling 필요.
- Dense vector에서 MRC 밀집도가 높은 곳이 있어서 negative sampling을 더 빡세게 해야할듯. **top-k에서 유사도 높은 벡터를 nagative하게 잘 흩어내야할듯.**
- 실제 wikipedia 데이터(모집단)의 길이 분포를 보고 이 분포와 비슷하게 kor_quad에서 샘플링해오는게 좋을 것임
- post-processing : 마지막 토큰이 조사일때 해당 부분을 제거하면 EM이 오르지 않을까?

### Dense Retriever

* build faiss 등의 기존 메서드 잘 수정해서 넣을 것
* top-k retriever, post_processing 바꾸어야함.

## (성익) Special token 추가가 필요한가?
* special token이 entity를 통으로 포착하는데에 효과가 있어 모델의 데이터 이해에 도움을 줄 수도 있다.
* special token이 unk token을 대체하는 방식으로 모델을 학습시킬수도 있으나 학습 시간이 너무 오래 걸릴 수 있다.
* 현재 우리가 수행하고있는 ODQA에서는 domain-specific한 영역이 없으므로 special token 추가에 따른 성능 향상이 거의 없을 것이다.
* [추가 조사](http://boostcamp.stages.ai/competitions/4/discussion/post/199)


## 논의할 것

* baseline code 완성되었으니, ~~짝코딩 형식으로~~ 팀 나눠서 피쳐 구현
    * DPR : p/q encoder weight 저장해두고, query 들어오면 load 후 q embedding만 해서 질의. sparse retriever와 형태 비슷
    * tokenizer : 함수 내부에서 호출이 아니라 class attribute로 load해서 사용?
        * mecab으로 형태소 분석 후 wordpiece tokenizer 사용.
            * multilingual 모델의 경우 (wordpiece를 사용하므로) 위와 같은 방식이 한국어에서 더 좋은 토크나이징 성능을 기대할 수 있음.
            * koBERT 등 sentencepiece tokenizer를 사용하는 모델은 mecab 사용 안해도 됨
        1. sentencepiece 사용 
        2. mecab + wordpiece 사용으로 분기할것.

## 코드 리뷰

* debug=True 옵션 시 dummy dataset 사용하도록 바꿀것
    * 의미론적으로 바꾸면 좋지만 안바꿔도 상관없음.

* tester.py에서 validation도 debugging 가능하도록 수정

* top-k negative sampling retriever, post_processing 바꾸어야함.

* run_mrc 함수
    * 모델별로 read 로직 분리하여 새로운 클래스로 만들자.
    * 모델만 바꿔가며 실험할 수 있도록 run_mrc + trainer 코드를 리팩토링 후 하나의 클래스로 만들자.

## Feature Assign
**(기한 : skeleton 오늘 안으로, 구현은 내일 안으로)**

- \<refactor\> Reader를 class로 refactoring -- 종헌, 건모
    - run_mrc 코드 리팩토링.
- \<refactor\> top-k sampling -- 성익
    - retriever 수정 / post processing(reader) 수정
    - retriever는 현재 top 1개의 context만 가져오고 있음
    - retriever를 top-k개의 context를 가져오는 방향으로 수정한 후, post processing(reader)에서는 top-k개의 context에서 정답 추출
    - retriever의 성능이 좋을 경우 top-k개의 context를 모두 가져와서 동등한 확률로 예측을 하는 방향이 문제가 될 수 있으므로 추후 높은 순위의 context에 가중치를 주는 방향으로 수정
        - 이는 실험해보고 성능 차이를 비교해본 후 적용할것.
- \<feat\> retriever 리팩토링 (+ DPR 클래스 구현) -- 수연, 지영
    - get_sparse_embedding() -> get_embedding() : general하게 method를 바꾸고, 파라미터를 받아 내부에서 embedding 방식(sparse, DPR)을 분기처리하자.
    - validation['contexts'], validation['questions']
        - document embedding encoder, wiki에 해당하는 [CLS] Token (1000, 784)
        - (1000, 784) * (784, 300) -> (1000, 300), 내적 유사도 순으로 뽑아오기
        - self.p_embedding에다가 값만 잘 집어넣어주면
        - 임베딩 저장이 안 되어 있을 경우
            - encoder 학습하는 로직 빼서 함수 구현
            - embedding.bin 저장
            - q_encoder.pth 저장
        
        - self.get_embedding(self)
        - self.get_query_doc_bulk(self)
        - retriever-reader구조


## 리뷰

* golden sample
    * 현재 강의에서 사용하고 있는 코드.
* bm25로 negative sampling, dense retriever golden sampling
    * 두개를 섞어 쓰는게 가장 효과가 좋다고 합니다.


## 멘토님께

- 가설관련 질문
- 사이드프로젝트 데이터셋, 관련 논문 등 질문