# 2주차 목요일 회의(0506)

## 이슈 정리

* 이슈를 할당할 레포?
    * Competition 관련된 것은 odqa_baseline에 넣기

* Done과 close의 구별
    * 팀단위로 한것은 리뷰하고 close
    * 개인이 한 것은 done되면 close해도됨

* #47
    * sentenecepeiece형 모델이 확실히 성능이 금방 오르고 잘 나옴.
        * xlm-robeta-large, KoBERT
    * (질문) dense는 왜 그정도의 성능이 나오지 않을까? topk를 1000이나 했는데..
        * 768차원으로 모델을 잡는것 자체가 너무 적은 feature로 파악하려는게 아닐까?
    * Dense Retriever 학습과정 코드 리뷰 필요.

* #44
    * 지금 당장 해야할지 dense부터 먼저 성능을 높이고 난 뒤 해야할 지 고민
    * hybrid 관련된 오픈소스 레포가 있어서 따라는 하고있는데 bm25 negative sampling을 못한다고 함.
    * top-k는 고려하지 않고 일단은 k=1 기준으로 만들고 있음.
        * 근데 negative sampling은 답만 1이고 나머지는 0인거 아닌가? 그럼 k를 고려할 필요가 없는거 아닌가?

* #24
    * in-batch negative와의 차이
        * in-batch negative는 유사하지 않은 문서들을 사용
    * bm25로 유사도가 높은 top-k 문단을 뽑아서 해당 샘플들 중 답인것은 1, 아닌것은 0으로 데이터 추가
        * 미리 특정 쿼리마다 어떤게 답인지 기록해두는것

* #41
    * 그냥 korQuAD 데이터셋 넣어서 돌렸을때는 성능이 오히려 떨어졌음.
    * 그래서 MRC 데이터셋과 KorQuAD 데이터셋 분포를 맞춰주기.

* #52
    * 질문과 답 사이의 연관관계
    * 겹치는 단어수 / 질문 총단어수의 비율이 대부분이 0.5이상(긍정적)
    * 어려운 문제의 경우
        * 직접적으로 단어가 드러나지 않고 대응되는 단어가 있는 문제
        * 아예 단서조차도 드러나있지 않은 문제
    * ngram을 늘려서 연속으로 나타난 단어 기준으로 EDA를 수행해봐도 좋을듯.
    * 결정적인 단서가 있는 지문의 비율이 어느정도인지 보면 좋을것같음.
    * SQuAD 데이터셋으로 ODQA를 수행하는 셈이라 이런건 버려야하지 않을까?



## 베이스라인 코드 논의할 점

* retriever를 config.json에 넣을 수 있도록 하이퍼파라미터 추가
    * retriever와 tokenizer를 다르게 하고 싶다면 추가적으로 하이퍼파라미터를 더 넣어줘야할듯

## 이번주 목표 설정

* (성익) #48 Wiki docs EDA + #40 reader loop for top-k

* (지영) #46 Hybrid Retrieval 구현

* 건모
    - #41 KorQuAD 데이터 셋 전처리
    - #49 Dense Retriever 성능 향상
* 종헌: #15 input data에 feature 정보 추가(DrQA) , #17 MRC Large model 돌려보기(성능 측정) 
* (수연) #47 BM25 성능 향상 실험 - Wiki preprocessing 바꿔보기, #24 Dense Encoder를 위한 데이터셋 구축