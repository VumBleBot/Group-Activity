# 2주차 금요일 회의(0507)

## EDA

- (건모) KorQUaD EDA
    - 우리 MRC 데이터셋에 비해 KorQuAD의 '1개 지문당 질문 수(질문 밀도?)'가 더 크다.  
    - 샘플링 분포 맞추어서 1만 4천개정도 남겼음.
        - 1배수(4천개 가량)로 저장할지? N배수(~1만 4천개) 저장할지?
        - mrc 데이터셋과 kor 데이터셋을 다 합쳐서 저장할지? 아니면 따로 두고 학습시에 concat해서 학습할지?
    - 추후 mrc 데이터셋/korQuad 데이터셋/ETRI 데이터셋 비율을 정해 추출하여 학습할 수 있도록 파이프라인 구축하는 것도 좋을 것 같음

- (성익) wikidocs EDA
    - title 별로 문서를 나누고 탐색
        - title은 주제 같은 역할 (확실한건 문서 제목은 아님)
        - 대부분의 문서가 title 1개당 문서 수가 5개 이하
            - 따라서 top-k에서 k는 5~10으로 주는 것이 합당한 것 같음
        - title과 문서 길이 간 상관관계는 거의 없다.
        - title 길이는 대부분 10~15 이하이다.
    - text 탐색
        - 길이는 보통 400~1000자
        - outlier(40000자 이상)가 극소수 존재
        - text에 한자만 가득한 경우가 있는데, 제외하는게 좋을듯
    - Retrieve
        - TF-IDF로 대부분의 문서를 찾아낼 수 있고 TF-IDF가 탐색하지 못한 것은 독해가 가능한 Dense retriever가 탐색 가능할 것
        - 다만 반대로 TF-IDF 기반 탐색이 쉬우므로 dense retriever의 역할이 그렇게 중요하지 않을 수도 있음
        - General한 질문들도 어느정도 찾아오므로 이게 꼭 큰 문제는 아닐수도 있음
        - 애매하게 희귀한 단어가 들어간 지문들은 TF-IDF가 강점을 발휘하지 못함.
        - 40-50%는 top-1, 75%정도가 top-3에서 추출 가능
        - TF-IDF 값이 낮은 단어를 제거해야할까?
    - BM25
        - BM25의 경우 일반적으로 TF-IDF에 비해 성능이 좋다.
        - 특히 잘 나오지 않는 단어(고유명사 등)를 잘 찾아낸다.
        - top-3 내에서 대부분의 지문을 찾아낼 수 있음
    - 그 외
        - 지문은 똑같은데 띄어쓰기 등이 달라 다른 context로 인식되는 경우도 존재

## (지영) Hybrid Retriever(Dense + Sparse)

- dense와 sparse의 docs score 스케일링이 안되어있음
    - dense는 70전후, dparse는 한자릿수~10전후
    - sparse가 훨씬 더 조밀하게 되어있어서 이를 활용해볼 수 있겠다.
    - 지금은 alpha로 가중치를 두었음.
    - 정규화를 안했는데도 성능이 좋게 나오는것은 의아함. sparse에 0.1 비중을 줬는데, 그렇다고 하더라도 dense의 표준편차가 너무 적어서 sparse가 거의 모든 역할을 다하는듯.
        - 성능이 좋게 안나온것 같은데.. top-100에서 70~80%이면 별로 안좋은것같다.
        - dense 자체가 애초에 784차원 내적값 기준으로 만들어져서 초기 initialization값을 생각했을때 계속 70전후로 놀고있는게 아닌가 한다.
            - 차원을 늘리면 doc score가 올라갈거라서 일단 정규화가 필요할 것 같다.
    - 건모님이 dense 돌려봤을때 성능?
        - 정성평가로 봤을때 적당히 잘 찾는거같아서..
        - 근데 실제로 top-k 찍어보니까 그정도가 아닌거같다.
    - 그래서 해결법
        1. nagative sampling으로 retriever 학습
        2. encoder 차원 수 늘리기

- 기존 dense retriever의 get_related_doc_bulk 파트에 문제가 있는 것 같음.
    - 이부분을 못잡아서 성능이 안나오는건 아닌가?

## (수연) ground_truth label 추가한 데이터셋

- 지금 한 부분이 메모리 비효율적일 수 있음.
- 이슈로 옮겨서 논의.

## (종헌) large model 성능 공유

- xlm-roberta large가 base 모델 기준으로 LB, Eval 기준으로 EM 3퍼센트 정도 차이.
- 근데 애초에 retrieve가 제대로 못가져오는 시점에서 크게 의미없는 스코어일수..

## Todo

- (수연,성익) wiki pre-processing
    - title별로 문서 묶어서 search-space 축소
    - 질문에 한자/고대한국어/외국어 있는지 확인 후, 없다면 문서에서 한자 없애기/한자가 많은 문서 아예 제거
    - "활나라"같이 주요 단어가 tokenized 되는것을 어떻게 찾아봐야하는지?
        - unused token 사용하는건 어떨지?
        - test dataset의 query를 찾아보기

- EDA도 좋지만 논문 위주로 휴리스틱한 방법을 찾아보는것도 시도해봐야할듯.
    - BM25와 같이 쓰되, BM25가 믿을만한 거면 쓰고, 아니면 dense retriever
    - score에 threshold를 하이퍼파라미터로 둠.(넘으면 BM25, 아니면 Dense)
    - sklearn의 LinearRegression 라이브러리 사용하면 됨..
    - 또는 score가 위에서부터 내려오다가 훅 뛸때(diff가 클때) dense를 쓰는것?
        - 근데 이건 너무 if else니까, 둘을 조합해서 쓰고.. 가중치를 바꾸어보는것도?

- (건모, 성익) Top-k reader bulk

- (건모) 수연, 지영 pull request 리팩토링

## 주의할것

- 주말에 심심해서 논문 읽으시는 분(...)은 Todo에 올려서 안겹치게 하기.
- 실험 필요할때 config 올려서 돌려달라고 하기
