# 3주차 목요일 회의(0513)

## 공유할 것

- (성익) EM 정답 기준이 생각보다 깐깐함
    - `이회창` != `이회창 후보`
    - 정확하게 그 문장에 있는 답을 찾아내야하는거 아닌지? 그럼 answer sentence selection이 중간에 한번 더 들어가는건 어떤지?
        - (건모) 한번 간단하게 시도해보고 EM이 오르면 더 파고, 아니면 버려도 될것같다
        - (종헌) 좀 부정적이다. 문장을 가져온다고 해도 과연 정확하게 잡을 수 있을까?

- (종헌) 정답군 candidate에서 어떻게 EM을 높일 수 있을까?
    - top n개에서 2번이 나온 것을 정답으로 그냥 올려버릴까?(nbest voting) - 좀 위험하긴 함
    - 정답 형식이 명확하지 않다. 수식어를 붙어야하는경우도 있고 떼야 하는 경우도 있고.. 일관적이지 않다.
        - 아무리 생각해도 지금 모델의 이해도는 좋은것같은데 EM을 어떻게 개선할 수 있을까?

## Reader 발전방향

- LSTM을 start logit과 end logit에 하나씩 할당하는 방법
- average pooling vs linear layer?
    - LSTM이 bidirectional이라서 양쪽 정보를 다 볼수있다고 생각했는데...
    - logit으로 보든 sigmoid를 써서 확률로 보든..
- LSTM에서 최종 output vector만 가져와서 logit으로 활용

## PR

- (수연) #67 Remove duplications when retrieving topk
    - hybrid에서 sparse topk, dense topk 뽑아오기
    - topk 제한을 풀면 계산량이 너무 많아져서 2배로 제한
    - fuzzyratio 0.5는 생각보다 엄청 높은 수치라 0.5로 문서 유사도 검사.
        - 불안정할거같으니 0.65로 올려보자.
    - topk*2배 뽑았는데 prediction에 넘겨줄만큼 충분한 개수가 안나오는지 assert로 체크 혹은 한번 더 뽑기

- (지영,건모) 논문 기준 feature 뽑는 방법
    - Fbound : feature 개수
    - kbound : topk
    - 노랑이 기존, 보라색이 바뀐것..?
    - topk를 뽑을때 128개를 뽑아서 doc scores가 내림차순으로 정렬되도록 해놨는데, 2의 배수로 증가해서 2, 4, 8, 16...의 평균값을 뽑아서 시각화한것
    - feature vector를 봤을 때 bm25를 써도 될거같을때 1, 아니면 0이 나와서 dense와 sparse의 threshold를 설정해주는것.
    - feature vector가 얼마나 군집이 잘 되는지를 보려고 했음
        - 파란색 / 노란색이 다른 군집 양상이 된다고 가정하면 logistic regression을 써도 된다 생각할 수 있음
        - feature 2개는 안된다. 
    - 같은 feature 내에서는 kbound가 높을 수록 더 잘 분리되는듯.

## 조교님이 주신 질문에 대한 답변

- 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?
    - 딥러닝 ⊂ 머신러닝 
    - 딥러닝은 신경망/역전파를 사용한다.
    - CPU/GPU 활용의 차이
    - 딥러닝은 심층신경망을 사용하게 되면서 기존의 머신러닝 모델에 비하여 블랙박스 속성이 강해졌다.
- 왜 갑자기 딥러닝이 부흥했을까요?
    - 데이터의 양이 많아지고 수집 경로도 다양해짐 (딥러닝은 머신러닝에 비해 데이터 크기가 커질수록 성능이 향상하는 경향)
    - GPU 성능 향상으로 큰 모델 구동이 가능해짐
    - 역전파 알고리즘이 잘 동작하게 됐습니다. ( 잘 초기화하는 방법을 발견함 )
    - ( 이건 엄청 중요하지는 않지만 ) 이름을 딥러닝으로 지은 것도 어느 정도 영향이 있다고 합니다.. ㅎ
    - 인간의 편향을 믿을바에야 컴퓨터의 연산 과정을 믿자는 의도에서
    - 사람이 일일이 feature를 뽑아낼 필요가 없었기 때문에
- ReLU의 장/단점? (연산, 수렴속도 측면은 어떤가요?)
    - Activation 계산이 굉장히 간단합니다.
    - gradient vanishing 현상이 sigmoid/tanh에 비해 없다.
    - 0 이하의 값은 모두 gradient가 0이 되어 많은 정보가 소실될 수 있다
- ReLU 이후에 나온 activation function들 아는 것
    - GeLU
    - Leaky Relu
    - SeLU
- 왜 bias가 필요할까요?
    - 실세계에 존재하는 데이터셋에 이미 bias가 존재하기 때문입니다.
    - bias를 측정할 수 있으면 예측한 값과 실제값의 차이를 bias만큼 조정해 실제값에 근사할 수 있다?
    - 데이터만으로 설명할 수 없는 부분을 모델에 포함시키기 위해
- 왜 bias라는 이름이 붙었을까요?
    - 모델의 예측으로부터 양으로 혹은 음으로 치우치게 만들어서(?)
    - bias => 편향 => 전체를 이동시키는 느낌..?
        - activation을 적용할 경계 대신 편향성을 이용하는 것 같음 
- CNN에서 Activation, Batch normalization, Convolution layer, Dropout, Maxpooling 배치순서
    - Batch Normalization - Convolution Layer - Maxpooling - Activation - Dropout
    - Dropout - Conv - BatchNorm - Activation - MaxPool
    - Dropout -> Batch Norm -> Conv -> Maxpooling -> Activation
    - (ResNet 기준) Conv - BatchNorm - Activation - MaxPool
- 학습에 메모리가 부족하다, 네트워크를 줄일 것인가 배치 사이즈를 줄일 것인가? (정답은 없고, 어떻게 의견이 다른 상대방을 나만의 논리로 설득할 수 있을지가 포인트입니다.)
    - 네트워크 크기를 줄이고, Batch Size는 gradient accumulation으로 해결한다. <- 한표, 두표
        - 네트워크 크기를 줄이는 이유는 학습 데이터의 영향을 키우기 위해서
    - 데이터가 소량인 경우 네트워크를 줄일 듯
- 트레이닝 데이터가 1000개, 테스트 데이터가 100000개이다. 최대 성능을 내기 위해 어떻게 해야할까? (위와 같습니다)
    - 트레이닝 데이터로 학습 후에
        - Confidence가 높은 데이터는 Pseudo Labeling
        - Confidence가 낮은 데이터는 직접 Labeling을 함으로써 Train 데이터의 절대적인 크기를 늘린다.
    - 기존의 분포를 고려하며 training data를 oversampling한다
        - augmentation?
    - 비슷한 task에 대해 비슷한 dataset으로 학습한 pretrained model을 찾아본다.
    - 직접 라벨링하여 트레인에 활용한다
    - 유사한 데이터를 활용하거나 minor한 data만 augmentation(oversampling)한다.
