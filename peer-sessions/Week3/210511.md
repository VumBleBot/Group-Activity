# 3주차 화요일 회의(0511)

## 멘토링

- wikipedia 데이터 셋 내부에 있는 한자라던가 개행문자들이 많던데 의미상으로 큰 의미없는 문자들이 많던데 그대로 두는게 좋을까? 전처리를 해야 할까?
    - 일반적으로 전처리를 하는 것이 좋다고 한다.
    - 개행 문자 같은 경우는 문단을 나누는 의미가 있을 수도 있다.
    - 한자 같은 경우는 한글이랑 어느 정도 관련있는 언어이므로 없애면 안된다.    
    - 조교님: 기본적으로 전처리를 하는 것이 맞다고 봅니다.
        - Main으로 해야 되는 것은 한글 모델링, 데이터 셋에는 여러 언어들이 섞여있었다.
        - 뺀게 최종적으로 성능이 가장 좋았다.
        - 뺀 이유는 추가적으로 온라인 러닝을 할 때는 전처리가 된 버전에서 튜닝을 하는 것이 성능이 좋아지더라
        - ETRI KorBERT vocab에는 불용어나 한자어같은게 남아있긴함. 그렇지만 보편적인 아이디어로는 제거하는게 더 좋다고 한다. -> 그렇지만 이번 competition은 좀 specific 할수도 있다. 테스트해봐야한다.
        - 만약 문맥을 이해하는데 한자나 개행문자 등이 도움이 되는 경우면 남겨두는게 좋을 것이다.
            - 많이 쓰이지 않는 한자와 많이 쓰이지 않는 한글이 같이 있다면 오히려 성능이 방해가 될것이다.
            - TODO: 한글, 한자가 많이 겹쳐져 있는지 시각화
        - task에 general하게 사용할 수 있는 모델이 오히려 task speicific한 모델보다 private score가 더 잘나오는 경우가 있다. --> 언어 자체를 더 잘 이해했기 때문에.

- tokenizer를 사용할 때 pretrained tokenizer를 그대로 사용하는게 좋을까? 종성분리를 하는게 좋을까?
    - 조교님 : 제 생각에는 안하는게 좋을것같습니다.
        - 만약 저게 진짜 의미가 있었다면 네이버나 카카오에서 이미 했겠죠.
        - 학술지의 수준과 저술자 수준을 보고 시도해보는게 좋습니다.
    - (질문) 최신 논문은 어떤 기준으로 유용함을 판단해야할까?
        - 계속 보다보면 눈에 보이는 저술자의 이름이 낯익은 논문이 있음.
        - 물론 새로운 사람이 스타가 될수도 있긴 한데, 유명학회 논문이나 유명저자가 쓴 논문만 보기에도 시간이 부족함.
        - 그래서 저는 원하는 분야의 lab을 track해서 해당 lab에서 논문을 발표할때마다 찾아서 읽는 편입니다.
        - 한국어 관련 논문들은 네이버, 카카오, 카이스트 같은 곳에서 많이 내는편. 해외 학술지에서 논문도 꽤 나오니까 그런거 보세요.
            - 언어가 그렇게 중요하진 않아요. 언어에 따라 퀄리티가 accept가 달라지지도 않고.
        - 국내에서 NLP 연구하시는 분들 것 publication에 검색해보면 돼요.

- sklearn의 TfidfVectorizer에서 max_features를 50000으로 설정했을 때 성능이 더 떨어지는 이유?
    - 정확히 알 수는 없지만, 50000번째 근처(혹은 이후)에서 빈도수 자체가 단어별로 큰 차이가 없었기 때문이라고 추측 가능
    - search space를 축소한다고/확장한다고 성능이 오른다 라는건 알 수 없다. 보통은 작게 잡았을 때 성능이 잘 나올 뿐. 실제로는 주어진 데이터셋에서 실험을 통해 직접 성능을 봐야함

- NLP에서의 시각화툴?
    - 있을 것 같긴 함
    - 시각화 조교님들께 여쭤보고 공유드릴게요

- reader 모델 학습방법 관련 질문
    - reader는 max_sequence_length로 원문 document를 잘라서 가져간다.
        - 원래는 정답이 하나의 document에 있지만, 잘라지고 난 뒤에는 특정 subdocument만 정답을 가져가게 되는것이다.
    - 지금 우리 모델에서는 retreiver -> reader로 넘어갈 때는 topk개를 넘겨주게 되어있는데, topk를 늘렸을 때 정답률이 현저하게 떨어졌다.
        - reader가 subdocument들이 너무 많아져서 헷갈리는것 같다..?
    - 조교님 의견 : 개인적으로는 너무 복잡해져서 성능이 떨어질거같아요.
        - 데이터셋이 그것까지 학습할 정도로 양이 안될것같아요.
            - 샘플링한 korquad까지 합쳐서 1만 2천개..
        - 시도는 해봄직한 것 같긴한데, 잘될지는 잘 모르겠어요.
            - 성능이 비약적으로 상승할것같지는 않아요. 잘 됐을때 소폭상승할거같은데..
            - 아이디어는 좋은데, 그걸 실현하기에는 이 문제가 너무 light한 느낌(그정도까지 모델링을 해야하나?)
    - TODO : TOPK를 늘려도 reader가 헷갈리지 않도록 하고싶다.

- 조교님이 생각하시기에 적은 데이터셋에서 성능이 오를만한 방법은 뭐가 있을까요?
    - 저는 작은 데이터셋이라면 작은 모델들을 많이 써서 앙상블할거같아요(제일 먼저 시도할 것은).
    - 작을수록 모델의 complexity를 낮춰야해요.
    - 언어 모델링 자체가 원래 complexity가 큰 모델링인데, 작은 데이터셋에서는 complexity를 낮추어주어야해요.

- hybrid retriever 관련 질문
    - negative sampling 방식
        1. BM25로 topk를 뽑아서 진짜 정답은 1을 주고 나머지는 0을 줘서 negative sampling
        2. cosine 유사도를 뽑아서 제일 높은거에 1을 주고 나머지는 0을 주기. 
    - CLS 토큰으로 뽑은 dense retriever가 BM25가 기존의 데이터 분포와 같을거같은데, 오히려 BM25로 뽑은 negative sample은 retriever가 판단하는 기준이 다르므로 데이터 분포가 다를것이다.
    - 조교님 : 지금 기존의 베이스라인코드가 어느정도 확보가 되어있다면 시도해보는게 좋을것같아요.

- word2vec 처럼 negative sampling해서 dense retriever의 embedding을 잘 형성하는 것같은데, 지금은 GPU때문에 batch size를 늘릴수가 없어요. 이것도 성능에 영향을 끼치나요?(지금 배치사이즈를 16까지 올릴수있어요 - BERT 모델 2개 기준)
    - 네. 끼치죠.
    - 16이면 못해볼 정도까진 아니에요. 좀 애매하긴한데.
    - 배치 구성 1-2개만 틀어져도 영향이 되게 크게 들어가다 보니까..
        - 배치에 영향을 크게 줄 요소들을 따로 묶을 수 있으면 좋을것같긴해요.
    - (질문) BERT model에 adamW 말고 momentum같은걸 추가하는게 더 좋을 수 있을까요?
        - 제 경험상으로는 배치가 작을때 손을 많이 대면 댈수록 성능이 떨어졌어요.
        - 만약 성능이 안좋다면 dropout을 크게 늘려보세요.

- document에 question을 여러개 조합하면 batchsize를 늘릴 수 있을것같은데..
    - 그건 좋은 아이디어같아요.
    - 데이터가 8만개쯤 되었으면 말씀하신 것들이 좀 더 안정적으로 나올것같긴한데..
    - 공개된 모델을 fine-tuning하는 셋으로도 너무 적은 데이터라.
    - 그래서 점수만 생각하면 시도할 수 있는게 그렇게 많지는 않아요.
    - 혹시 이 데이터셋에서 성능이 안올랐는데 다른 big dataset에서는 크게 오를 수 있을거같아요. 
        - 그래서 코드를 다 들고 나중에 많은 데이터셋을 얻게 되었을때/실험/연구를 하게되었을때 꼭 테스트해보세요.
    - (질문) question은 korquad 데이터셋의 question을 쓰면 안되겠죠?
        - 그건 안되겠죠. 분포가 달라지니까.
    - 강화학습 쪽에서는 분포를 바꾸는 기법이란게 있어요.
        - **importance sampling**이라는 개념
        - 복잡한 분포를 단순한 분포로 바꿀 수 있는 개념인데, nlp에서도 쓸 수 있는 지는 모르겠어요.
        - 한번 키워드를 찾아보셔도 좋을것같아요.

- ODQA 첫 논문이 LSTM을 사용했었는데, word embedding말고도 여러 토큰들을 넣어줬었습니다. position embedding으로 EM token을 BERT에서 만들어서 LSTM으로 넣는 방식은 어떻게 생각하시는지?(질문이 이거 맞나요?)
    - 저는 약간 부정적입니다.
    - 저는 positional encoding이 그렇게까지 좋은지는 잘 모르겠어요.
        - (종헌) 일반적인 positional encoding이랑은 다른거 같다고 생각합니다. 다만 휴리스틱하게 생각해본 방법이라 근거자체는 의문인 상태라서..
        - (종헌) 사실 기존의 논문에서는 LSTM 하나만 활용해서 했기 때문에 정보가 부족하다고 생각하고 여러토큰들을 넣어준것같은데.. 트랜스포머가 나온 시점에서 그럴 필요가 있을지는 또 모르긴 합니다.
    - LSTM 하나가 더 적은 양의 정보를 처리하되 여러 LSTM을 둬서 최종적으로 많은 양의 정보를 보게 한다면 그건 해볼만한 것 같아요.
    - 배치사이즈 16으로 BERT + LSTM 2개까지 돌릴 수 있나요?
        - 될것같다면 저라면 해볼것같아요.
    - (질문) 1,0 EM 바이너리 값을 넣어주는게 그렇게까지 크게 영향이 있을까?
        - 영향이 크지 않기 때문에 더 시도해볼만한 것 같아요.
        - 너무 영향이 크지 않아서 부담이 적으니까
    - 아이디어 괜찮은 것 같습니다!

- 기술면접 PT
    - 취업 준비하시는 분들에게 기술면접 PT를 어떻게 하는지. 프로젝트 발표같은것도 필수로 있었음.
    - 조교님의 과거 면접 시뮬레이션(학부졸업했을 때 기준)
    - 면접관이라면 어떤 질문을 할 지 우리가 질문을 해보도록...
    - 좋은점은 취하고 아쉬운 점은 반면교사 삼도록!

## 이번주 역할 분배

Reader와 Retriever 각각 맡아서 각자의 파트를 발전시키는 방향으로 해보자!

- 데이터 전처리 - 성익
    - 일반적인 전처리
        - 한자는 outlier만 제거하기로(ex-소학언해같은)
    - title과 context concat
    - 시각화 - 건모
    - ETRI 데이터셋 EDA 및 데이터셋 Sampling
        - 문서길이에 따라 class imbalance 문제.. 체크
        - 구조 좀 바꾸어야하는 파트가 있을듯

- Retriever - 수연, 지영
    - 적은 Topk로 성능을 높이는것이 중요하다! 입출력 형식만 잘 맞춰주세요.
    - Sparse 
    - Dense - 건모
    - Hybrid
    - new retriever..?
        - 오픈소스 이용하여 기존의 입출력 형식 맞추어서 새로운 구현 추가.
        - score가 matmul이 아니라 벡터유사도 구하는 다른 라이브러리 함수가 구현되어있음.

- Reader - 종헌
    - 성능이 높은 것 같아도 그렇게 높은게 아니다. context가 주어진건데도 60%정도면.. reader 모델도 성능을 많이 올려야한다.
    - ETRI KorBERT finetuning하여 적용
        - offset mapping 
    - 뒷단에 LSTM 레이어 추가(n개)
    - 논문의 sliding window 구현

- 앙상블 파이프라인 (4주차)
    - utils_qa에서 사용하는 logit값을 soft voting하여 앙상블 하도록

## 테크톡

1 - 2 - 1 순으로 들으세요😍