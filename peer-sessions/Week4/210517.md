# 4주차 월요일 회의(0517)

## PR 검토

- #87 custom head를 가진 reader model 코드 추가 + masking logic
    - Backbone model에 custom LSTM, CNN, FC head 추가 시도(+ 5/16 Complex CNN head 추가)
    - CNN, FC는 성능이 비슷하게 나옴
    - 학습 중 masking 추가 시도 -> 성능 오히려 하락
        - `masking_ratio` traing argument 추가 -> custom에서만 동작, DPR에서는 동작하지 않음
        - SpanBERT에서 영감을 받아서 연속된 2개의 토큰을 마스킹하도록 되어있으나 추후 어떻게 해볼지 아직 고민중
    - training 중에 evaluation(EM, F1) 기능 추가
        -`do_eval_during_training`을 training args에 추가
    - model_name_or_path로 체크포인트를 바로 불러오도록 하는 기능 구현 아직 완성 안됨.
        - DPR 말고 다른 모델 checkpoint를 쓸거면 model_path를 명시해주어야하고, 체크포인트 사용하지 않을거면 공백으로 둘 것
    
- #89 ensemble용으로 utils_qa 리팩토링
    - utils_qa에서 기능별로 함수 분리
    - ensemble.py가 나오고 나면 다시 리팩토링할수도

## 이슈 정리

- random masking 구현
- CLS 토큰 사용 포기
- ColBERT 구축 했으나 용량이 너무 커서 사용 불가
- question,document pair 데이터셋 하이퍼파라미터를 못찾아서 성능이 안나옴
- #60 보면 좋을것같은 논문 리스트
    1. 토큰 레벨에서 interaction하는 논문
        - ColBERT같은거라 용량 많이 먹을것같아서 pass
    2. 데이터 augmentation 후에 어떻게 training해야 할지 다룬 논문
        - 주제에서 먼거부터 학습시킬지? 아니면 섞어서 학습시킬지 등등
- #84 BM25 추가 구현
    - 구현하고 아직 merge하지 않았는데, 기존 성능과 비슷비슷해서...
    - 비슷비슷해서 합쳐서 score내볼까? 했는데 비슷하게 나와서..
    - Improvements to BM25 and Language Models Examined(논문)
        - 3.4부터 수식이 좀 복잡해져서..
        - 수식 구현하고싶으신 분은 해주세요...
        - BM25 adpt : 데이터셋 8개 9개를 사용했을 때 5개에서 최고 성능이 나와서 구현해보고 싶음

- #90 DPRElectra
    - k=20까지 해봤는데 acc 오르는게 미미
    - hyperparameter나 batchsize 문제일수도..

## 질문

- (지영) haystack 오픈소스 사용하여 Retriever 성능평가
    - 240개 validation dataset에 대해서 query * 6만개 docs에 대해서 matmul하여 성능을 그래프로 그려볼것(기존에 run_retrieval에 있던것처럼)
    - k가 늘어남에 따라서 어떻게 변화하는가를 봐야하므로 k값을 1부터 10까지 변화시키면서 확인할것

## 조교님께 질문할 것들

- 질문 리스트 답변했던거 첨삭
- reaer 관련 질문
    - backbone마다 성능 차이가 너무 많이나는 점
    - custom head를 바꿔도 성능 차이가 없는 경우
        - backbone이 달라지면 custom head의 편차도 심함
    - (건모) 학습시에 backbone이 올바른 정보를 head에 넘겨주지 못해서 성능이 안나오는것같다. 그래서 feature를 좀 더 추가해주는게 맞을 것 같다.
        - 그래서 EM token을 추가해주려고 했는데 어떻게 넣을지 그걸 고민중...
        - embedding layer 추가 이외에 다른 방법이 있을지?
